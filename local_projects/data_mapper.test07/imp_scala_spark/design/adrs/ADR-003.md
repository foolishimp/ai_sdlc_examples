# ADR-003: Either-Based Error Handling

**Status**: Accepted
**Date**: 2026-02-23
**Implements**: REQ-TYP-03, REQ-TYP-03-A, REQ-TYP-04, REQ-ERROR-01, RIC-ERR-01

## Context

The CDME specification mandates that "Failures are Data" (Axiom 10). Specifically:

- Errors must be modeled as `Left(Error)`, valid data as `Right(Value)` (REQ-TYP-03)
- Failures must not short-circuit execution unless a batch threshold is exceeded (REQ-TYP-03-A)
- Error handling must be idempotent (REQ-TYP-04)
- Error objects must carry diagnostic information: failed constraint, offending value, source entity/epoch, morphism path (REQ-ERROR-01)
- A circuit breaker must detect systemic failures early (RIC-ERR-01)

We need an error handling strategy that satisfies all these requirements while being composable, testable, and compatible with Spark's distributed execution model.

## Decision

Use `Either[CdmeError, A]` as the universal error type throughout the CDME codebase. No exceptions are thrown in business logic.

### Core Principles

1. **All public methods return `Either[CdmeError, A]`**. No method throws exceptions for business logic failures. JVM-level exceptions (OutOfMemoryError, StackOverflowError) are not caught.

2. **`CdmeError` is a sealed trait hierarchy** (aligned with ADR-001) carrying all diagnostic fields required by REQ-ERROR-01:
   ```scala
   sealed trait CdmeError {
     def code: String
     def message: String
     def sourceEntity: Option[EntityId]
     def sourceEpoch: Option[EpochId]
     def morphismPath: List[MorphismId]
   }
   ```

3. **Composition via for-comprehension**:
   ```scala
   for {
     plan   <- compiler.compile(input)
     result <- runtime.execute(plan, context)
   } yield result
   ```

4. **Record-level bifurcation**: During execution, each record is processed through an `Either` fork. `Right` records proceed; `Left` records accumulate in the Error Domain. The error accumulator is separate from the control flow — a single `Left` does not stop the pipeline.

5. **Batch-level short-circuit**: The `FailureThreshold` (REQ-TYP-03-A) is checked after each batch of records. If exceeded, the pipeline halts and returns a `Left(BatchThresholdExceeded(...))`.

6. **Circuit breaker**: For the first N records (configurable), the error rate is monitored. If it exceeds the threshold (e.g., 5% of first 10,000 rows), execution halts immediately with a `Left(CircuitBreakerTripped(...))`, preventing mass DLQ overflow.

### Error Accumulation Strategy

For record-level processing, we use a dual-stream approach:

```scala
trait ErrorRouter[F[_]] {
  def bifurcate[A](
      records: F[A],
      validate: A => Either[CdmeError, A]
  ): (F[A], F[ErrorRecord])
}
```

This returns both the valid stream and the error stream, enabling continued processing of valid records while capturing all errors.

### Spark Compatibility

In the Spark layer, `Either` is materialized differently:
- Records are processed as DataFrames
- Error routing uses `Column` expressions (`when`/`otherwise`)
- Error records are collected into a separate DataFrame
- The `Either` abstraction lives at the morphism boundary, not at the Spark operation level

## Consequences

### Positive

- Failures are first-class data, never silently dropped — directly satisfying Axiom 10
- Idempotency of failure (REQ-TYP-04) follows naturally from pure functions returning `Either`
- Composable via `for`-comprehensions and `flatMap` chains
- Error objects carry full diagnostic context per REQ-ERROR-01
- Circuit breaker prevents DLQ overflow on systemic failures
- Testable: every error path is a `Left` value that can be asserted against

### Negative

- Verbose compared to exception-based code (mitigated by `for`-comprehensions and helper methods)
- Spark integration requires translation between `Either` and DataFrame error columns — an impedance mismatch at the boundary
- Accumulating errors requires explicit collection, unlike exceptions which propagate automatically
- Developers must be disciplined about not throwing exceptions in business logic

### Risks

- If third-party libraries throw exceptions (e.g., Spark internal errors), they must be caught at integration boundaries and wrapped in `CdmeError`. Incomplete wrapping could leak exceptions.
- Error accumulation in large datasets may consume significant memory. Mitigation: the circuit breaker and batch threshold limit accumulation.

## Alternatives Considered

1. **Scala `Try` monad** — Short-circuits on first failure, violating REQ-TYP-03 (no short-circuit unless threshold exceeded). Rejected.
2. **Cats `Validated` (applicative)** — Accumulates errors but cannot compose sequentially via `flatMap`. Useful for validation but not for pipeline execution. Partially adopted: the compiler uses `Validated` for accumulating compilation errors; the runtime uses `Either` for execution.
3. **ZIO error channel** — Powerful but introduces a heavy dependency and paradigm shift. Rejected for simplicity and Spark compatibility.
4. **Exceptions with DLQ pattern** — Standard in traditional ETL but violates Axiom 10 and makes testing harder. Rejected.

## References

- [ADR-001](ADR-001.md) — Sealed ADT Type System (CdmeError hierarchy)
- [ADR-006](ADR-006.md) — Spark Integration Strategy (Either-to-DataFrame boundary)
- [ADR-008](ADR-008.md) — Testing Strategy (testing error paths)
- REQ-TYP-03 — Error Domain Semantics
- REQ-TYP-03-A — Batch Failure Threshold
- REQ-TYP-04 — Idempotency of Failure
- REQ-ERROR-01 — Minimal Error Object Content
- RIC-ERR-01 — Probabilistic Circuit Breakers
- Specification Axiom 10 — Failures are Data
