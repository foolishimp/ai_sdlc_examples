# ADR-013: Scala 2.13 over Scala 3

**Status**: Accepted
**Date**: 2026-02-23
**Implements**: (Technology binding — impacts all requirements)

## Context

The CDME technology stack specifies Apache Spark 3.5.0 as the distributed execution runtime (see CLAUDE.md). Spark 3.5 requires a specific Scala version for binary compatibility:

- **Spark 3.5.x** is published for Scala 2.13 and Scala 2.12
- **Spark 3.5.x** is NOT published for Scala 3

Scala 3 (Dotty) introduces significant language changes:
- New `enum` syntax for ADTs (replaces sealed trait + case class pattern)
- `given`/`using` replacing `implicit`
- Union types, intersection types, opaque types
- Simplified type-level programming

We must choose between Scala 2.13 (Spark compatible) and Scala 3 (modern features but no Spark support).

## Decision

Use **Scala 2.13.12** for the entire project.

### Rationale

1. **Spark 3.5 binary compatibility**: Spark 3.5.0 is built against Scala 2.13. Running Spark with Scala 3 requires Scala 3's backward compatibility mode (`-Xtarget:2.13`), which is experimental and not recommended for production. Using Scala 2.13 natively avoids this risk.

2. **Ecosystem compatibility**: The broader Spark ecosystem (Delta Lake, Iceberg, Hudi, spark-testing-base) targets Scala 2.13. Using Scala 3 would limit connector availability.

3. **Sealed ADTs work in 2.13**: The core CDME design pattern (sealed trait hierarchies with exhaustive matching, ADR-001) works identically in Scala 2.13. Scala 3's `enum` syntax is syntactic sugar over the same mechanism.

4. **`Either` monad works in 2.13**: The error handling strategy (ADR-003) uses `Either`, `for`-comprehensions, and `flatMap`, all of which are standard in Scala 2.13.

### Impact on Type Encoding

| Feature | Scala 3 | Scala 2.13 Equivalent |
|---------|---------|----------------------|
| `enum CdmeType` | Built-in | `sealed trait` + `case class/object` |
| Exhaustive matching | Built-in | `-Xlint:_ -Xfatal-warnings` |
| Union types | `A | B` | `Either[A, B]` or sealed trait |
| Opaque types | `opaque type EntityId = String` | `final case class EntityId(value: String) extends AnyVal` |
| Given/using | `given Ordering[Grain]` | `implicit val grainOrdering: Ordering[Grain]` |
| Extension methods | `extension (g: Grain)` | `implicit class GrainOps(g: Grain)` |

All Scala 3 features used in the CDME design have well-established Scala 2.13 equivalents. The migration path to Scala 3 is straightforward when Spark supports it.

### Compiler Flags

To maximize safety in Scala 2.13:

```scala
scalacOptions ++= Seq(
  "-Xlint:_",                    // Enable all lints
  "-Xfatal-warnings",           // Warnings become errors
  "-deprecation",               // Warn on deprecated API usage
  "-unchecked",                 // Warn on unchecked type operations
  "-feature",                   // Warn on advanced features
  "-Wunused:imports,privates",  // Warn on unused imports/privates
  "-Wvalue-discard"             // Warn on discarded values
)
```

These flags ensure:
- Non-exhaustive pattern matches on sealed traits are compile errors (critical for ADR-001)
- Unused imports/variables are flagged
- Implicit conversions are flagged if unintended

## Consequences

### Positive

- Full Spark 3.5.x compatibility without experimental modes
- Full ecosystem compatibility (Delta Lake, Iceberg, all Spark connectors)
- Stable, well-understood language semantics
- Large body of existing Scala 2.13 + Spark documentation and examples
- No risk from Scala 3 migration issues (TASTy compatibility, binary format changes)
- Straightforward future migration to Scala 3 when Spark supports it

### Negative

- No access to Scala 3's union types (useful for error handling)
- No opaque types (value classes with `extends AnyVal` have limitations: single field, no nesting)
- More verbose ADT syntax (sealed trait + case class vs. enum)
- `implicit` can be confusing (but well-understood by Scala developers)
- Scala 2.13 is in maintenance mode (security fixes only); Scala 3 receives active development

### Risks

- Scala 2.13 may reach end-of-life before Spark supports Scala 3. Mitigation: Spark 4.0 (expected 2025/2026) is planned to support Scala 3.
- Developer preference may shift to Scala 3, making it harder to attract contributors. Mitigation: the migration path is well-defined.

## Alternatives Considered

1. **Scala 3 with `-Xtarget:2.13`** — Experimental cross-compilation mode. Risk of subtle binary compatibility issues with Spark. Rejected for production use.
2. **Scala 2.12** — Also supported by Spark 3.5 but lacks Scala 2.13 improvements (collections rewrite, `Either.toOption`, better type inference). Rejected.
3. **Kotlin on JVM** — Good interop with Spark Java API. Loses Scala's type system expressiveness (sealed hierarchies, pattern matching, higher-kinded types). Rejected.
4. **Java 17** — Maximum Spark compatibility but loses all functional programming features that make the CDME design clean. Rejected.

## References

- [ADR-001](ADR-001.md) — Sealed ADT Type System (ADT encoding in 2.13)
- [ADR-003](ADR-003.md) — Either-Based Error Handling (Either in 2.13)
- [ADR-006](ADR-006.md) — Spark Integration Strategy (Spark version dependency)
- [ADR-007](ADR-007.md) — Multi-Module sbt Structure (scalaVersion setting)
- CLAUDE.md — Technology Stack (Scala 2.13.12, Spark 3.5.0)
- Spark 3.5.0 documentation — Supported Scala versions
