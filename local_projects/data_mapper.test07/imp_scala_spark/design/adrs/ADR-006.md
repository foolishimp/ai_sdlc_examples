# ADR-006: Spark Integration Strategy

**Status**: Accepted
**Date**: 2026-02-23
**Implements**: REQ-PDM-01, REQ-PDM-02, REQ-PDM-02-A, REQ-PDM-03, REQ-PDM-05, RIC-SKW-01, RIC-PHY-01, RIC-AGG-01

## Context

The CDME specification requires physical storage abstraction (REQ-PDM-01): re-pointing a logical entity from one storage medium to another must not require changing business logic. The LDM-to-PDM binding is explicitly described as a "Functor" (Axiom 2, Terminology Dictionary).

The technology choice is Apache Spark 3.5.0 (as specified in CLAUDE.md). Spark provides:
- Distributed DataFrame computation
- Rich type system via StructType
- Built-in aggregation, join, and partitioning operations
- Integration with diverse storage backends (HDFS, S3, JDBC, Parquet, Delta Lake)

We need to design the binding between the CDME logical layer and Spark's physical layer such that:
1. The binding preserves category structure (functor laws)
2. CDME types map cleanly to Spark types
3. Distributed execution leverages Spark optimizations
4. The binding is replaceable (another runtime could be substituted)

## Decision

Implement the LDM-to-PDM binding as a literal **functor** — a structure-preserving map from the logical category to the Spark physical category.

### The PhysicalBindingFunctor

```scala
trait PhysicalBindingFunctor {
  def bind(entity: Entity, pdm: PhysicalDataModel, epoch: Epoch)
          (implicit spark: SparkSession): Either[CdmeError, DataFrame]
  def mapSchema(entity: Entity): StructType
}
```

This functor preserves:
- **Objects**: `Entity` maps to `DataFrame` with a matching `StructType`
- **Morphisms**: Logical transformations map to Spark operations (joins, aggregations, column expressions)
- **Identity**: `Id_E` maps to identity transformation on DataFrame
- **Composition**: `g . f` maps to Spark operation composition

### Type Mapping

| CdmeType | Spark StructField | Notes |
|----------|------------------|-------|
| IntType | IntegerType | |
| FloatType | DoubleType | Spark uses Double, not Float, for precision |
| StringType | StringType | |
| BooleanType | BooleanType | |
| DateType | DateType | |
| TimestampType | TimestampType | |
| OptionType(T) | Nullable StructField of T | Spark columns are nullable by default |
| ListType(T) | ArrayType(T) | |
| ProductType | StructType (nested) | Spark supports nested structs |
| SumType | StructType + discriminator | Encoded as struct with `_type` tag field |
| RefinementType | Base Spark type | Predicate checked via UDF at runtime |
| SemanticType | Base Spark type | Label stored in StructField.metadata |

### Execution Model

1. **Source binding**: Load DataFrames from physical targets (Parquet, JDBC, API, etc.) via Spark readers
2. **Partition alignment** (RIC-PHY-01): Detect compatible partitioning; inject repartition when needed
3. **Skew mitigation** (RIC-SKW-01): Detect skew via sampling; apply salted joins for hot keys
4. **Morphism execution**: Translate morphisms to Spark operations:
   - Structural (FK): DataFrame join operations
   - Computational (Synthesis): Column expressions or UDFs
   - Algebraic (Fold): `groupBy().agg()` with monoid validation
5. **Error bifurcation**: `when`/`otherwise` expressions route error records to error DataFrame
6. **Adjoint capture**: Reverse-join metadata captured via `groupBy` side-output
7. **Sink writing**: Write output DataFrames to physical targets

### Skew Mitigation Strategy

```scala
trait SkewMitigator {
  def detectSkew(df: DataFrame, keyColumn: String, threshold: Double): Boolean
  def applySaltedJoin(
      left: DataFrame, right: DataFrame,
      joinKey: String, saltFactor: Int
  ): DataFrame
}
```

Salted join algorithm:
1. Add random salt column `[0, N)` to left side
2. Explode right side N times with corresponding salt values
3. Join on `(key, salt)` instead of just `key`
4. Remove salt column from result

### Approximate Aggregation (RIC-AGG-01)

Spark provides built-in approximate functions:
- `approx_count_distinct` (HyperLogLog)
- Custom UDAFs for t-Digest (percentiles)

These are registered as `ApproxAgg` morphisms, distinct from `ExactAgg`.

## Consequences

### Positive

- The functor abstraction keeps the CDME logical layer independent of Spark — another runtime (Flink, DuckDB) could implement the same interface
- Spark's distributed execution provides scalability to large datasets
- Type mapping is clean and reversible
- Skew mitigation is built into the execution model
- Partition optimization reduces unnecessary shuffles
- Rich ecosystem of Spark connectors (S3, JDBC, Kafka, Delta Lake) satisfies diverse PDM binding requirements

### Negative

- Spark's lazy evaluation and query optimizer can make debugging harder (logical plan vs physical plan vs execution plan)
- The `Either` monad from ADR-003 does not map directly to Spark — error bifurcation requires column-level `when`/`otherwise` patterns
- Spark's provided scope means the CDME cannot bundle Spark — it must run in a Spark-enabled environment
- UDF performance is lower than native Spark expressions — synthesis morphisms should prefer native expressions where possible

### Risks

- Spark version upgrades may break compatibility. Mitigation: Spark 3.5 is a stable release; pinning to 3.5.x.
- Spark's internal type system may not perfectly match CDME types for edge cases (e.g., arbitrary-precision decimals). Mitigation: document known type mapping limitations.
- Salted join increases data volume by factor N. Mitigation: salt factor is configurable; only applied when skew exceeds threshold.

## Alternatives Considered

1. **Apache Flink** — Better for streaming, but less mature for batch and less widely deployed. Rejected for initial implementation; could be added as alternative runtime later.
2. **DuckDB** — Excellent single-node performance, but does not support distributed execution. Could be used for testing and small datasets.
3. **Custom execution engine** — Maximum control but enormous development effort. Rejected.
4. **Spark Dataset[T] (typed)** — Type-safe Spark API. Rejected because it conflicts with schema-on-read patterns needed for runtime-loaded LDMs. DataFrames are more flexible for dynamic schemas.

## References

- [ADR-001](ADR-001.md) — Sealed ADT Type System (CdmeType to Spark type mapping)
- [ADR-003](ADR-003.md) — Either-Based Error Handling (error bifurcation in Spark)
- [ADR-005](ADR-005.md) — Grain Ordering System (grain in Spark operations)
- [ADR-007](ADR-007.md) — Multi-Module sbt Structure (cdme-spark module)
- [ADR-013](ADR-013.md) — Scala 2.13 over Scala 3 (Spark version constraint)
- REQ-PDM-01 — Functorial Mapping
- REQ-PDM-02, REQ-PDM-02-A — Generation Grain
- REQ-PDM-03 — Boundary Definition
- REQ-PDM-05 — Temporal Binding
- RIC-SKW-01 — Skew Mitigation
- RIC-PHY-01 — Partition Homomorphism
- RIC-AGG-01 — Sketch-Based Aggregations
- Specification Axiom 2 — Separation of Concerns (LDM/PDM/Binding)
