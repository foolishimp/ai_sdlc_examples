# ADR-009: Spark as Reference Execution Binding

**Status**: Accepted
**Date**: 2026-02-20
**Feature**: REQ-F-CDME-001
**Drives**: REQ-NFR-PERF-001, REQ-NFR-PERF-002, REQ-DATA-QUAL-003

---

## Context

CDME must execute morphisms on distributed compute frameworks (REQ-NFR-PERF-001) with skew mitigation (REQ-NFR-PERF-002) and input schema validation (REQ-DATA-QUAL-003). The requirements specify "at least one distributed framework binding" with Spark as the reference implementation (ASM-002).

The design must be framework-agnostic at the model and compiler layers (per ADR-003), with Spark dependency confined to an integration module.

## Decision

Define an abstract **`ExecutionEngine`** trait in the runtime layer. Provide **Apache Spark** as the reference implementation in a separate `cdme-spark` module. Other frameworks (Flink, single-threaded local) can be added later by implementing the same trait.

```scala
// runtime layer (no Spark dependency)
trait ExecutionEngine:
  def execute(artifact: ExecutionArtifact): Either[ExecutionError, ExecutionResult]
  def dryRun(artifact: ExecutionArtifact): Either[ExecutionError, DryRunResult]

// integration/spark layer (depends on Spark)
class SparkExecutionEngine(spark: SparkSession) extends ExecutionEngine:
  override def execute(artifact: ExecutionArtifact): Either[ExecutionError, ExecutionResult] = ...
  override def dryRun(artifact: ExecutionArtifact): Either[ExecutionError, DryRunResult] = ...
```

## Alternatives Considered

### Alternative 1: Spark-First (Direct DataFrame API)

Build directly against Spark DataFrames throughout the codebase, with no abstraction layer.

**Pros**: Simpler initial implementation. Full access to Spark optimizations (Catalyst optimizer, Tungsten, etc.). No abstraction overhead.
**Cons**: Entire codebase depends on Spark. Cannot test without a Spark session. Cannot run locally without Spark. Model layer would depend on Spark (violating zero-dependency constraint). Framework migration would require rewriting everything.

**Rejected because**: Violates project constraints on model layer independence. Makes testing and local development impractical.

### Alternative 2: Apache Flink as Primary

Use Flink instead of Spark as the reference framework.

**Pros**: Better streaming semantics. Lower latency for event-driven processing. Native exactly-once guarantees.
**Cons**: Smaller ecosystem in the enterprise data space. Fewer Scala 3 libraries. CDME is batch-oriented (epoch-based), which is Spark's strength. The requirements assume Spark (ASM-002). Flink's DataStream API is more complex for batch use cases.

**Rejected because**: CDME is epoch-based (batch); Spark is the stronger fit. Requirements explicitly assume Spark availability.

### Alternative 3: Custom Distributed Engine

Build a custom distributed execution engine tailored to CDME's category-theoretic operations.

**Pros**: Perfect fit for CDME's semantics. No framework compromises. Optimizations for category-specific patterns (adjoint metadata capture, grain checking during execution).
**Cons**: Enormous implementation effort. No existing optimizer (Catalyst). No existing cluster management. No ecosystem integration. Would require years of engineering for reliability.

**Rejected because**: Impractical. Spark provides a mature, battle-tested distributed execution engine. Custom engine is unjustifiable when the abstraction layer allows Spark to be used without contaminating the core model.

## Consequences

**Positive**:
- Model and compiler layers are completely Spark-free
- Unit tests run without Spark (mock `ExecutionEngine` or local implementation)
- Spark dependency is confined to `cdme-spark` module
- Future framework additions (Flink, local) require only a new module
- Full access to Spark's optimizer and distributed execution within the integration layer

**Negative**:
- The `ExecutionEngine` abstraction must be general enough for multiple frameworks, which may not expose framework-specific optimizations
- Some Spark-specific patterns (broadcast joins, repartition hints) must be abstracted or configured externally
- Testing the Spark binding requires a Spark test environment

**Mitigations**:
- The `ExecutionEngine` trait includes an `ExecutionHints` parameter for framework-specific optimization suggestions
- Integration tests for Spark use `SparkSession.builder().master("local[*]")` for fast local testing

---

**Implements**: REQ-NFR-PERF-001 (distributed execution), REQ-NFR-PERF-002 (skew mitigation), REQ-DATA-QUAL-003 (schema validation)
