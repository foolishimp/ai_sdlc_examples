# ADR-012: Circuit Breaker with Sampling Pre-Phase

**Status**: Accepted
**Date**: 2026-02-20
**Feature**: REQ-F-CDME-001
**Drives**: REQ-DATA-QUAL-001, REQ-DATA-QUAL-002

---

## Context

CDME must support configurable batch failure thresholds (REQ-DATA-QUAL-001) and distinguish structural/configuration errors from genuine data quality errors via early-stage sampling (REQ-DATA-QUAL-002). The concern is that a misconfigured pipeline (e.g., wrong column mapping) would produce millions of identical errors, flooding the Error Sink and wasting compute resources.

The engine needs a fast-fail mechanism that detects likely configuration errors before processing the full dataset.

## Decision

Implement a **circuit breaker with a sampling pre-phase**. Before processing the full dataset, sample a configurable number of records (default 10,000), execute the full pipeline on the sample, and check the failure rate. If the failure rate exceeds a structural error threshold (default 5%), abort with a configuration error. Otherwise, proceed to full execution with the standard batch threshold monitoring.

```scala
case class CircuitBreakerConfig(
  sampleSize: Int = 10_000,
  structuralErrorThreshold: Double = 0.05,  // 5%
  enabled: Boolean = true
)

def preCheck(
  artifact: ExecutionArtifact,
  engine: ExecutionEngine,
  config: CircuitBreakerConfig
): Either[StructuralError, Continue] =
  val sample = engine.sample(artifact, config.sampleSize)
  val results = engine.executeOnSample(artifact, sample)
  val failureRate = results.errorCount.toDouble / results.totalCount
  if failureRate > config.structuralErrorThreshold then
    Left(StructuralError(failureRate, results.errors.take(10)))
  else
    Right(Continue)
```

## Alternatives Considered

### Alternative 1: No Circuit Breaker (Rely on Batch Threshold Only)

Process the entire dataset and halt when the batch threshold is exceeded.

**Pros**: Simpler. No sampling overhead. No false negatives from unrepresentative samples.
**Cons**: A misconfigured pipeline processes potentially millions of records before halting. Error Sink floods with identical errors. Compute resources are wasted. The batch threshold is designed for data quality errors, not structural errors -- a 100% failure rate due to a wrong column name should be caught instantly, not after processing 1% of a billion records.

**Rejected because**: Wastes compute and floods error sinks for configuration errors. The purpose of the circuit breaker is specifically to distinguish structural from data quality errors.

### Alternative 2: Schema-Only Pre-Check (No Data Sampling)

Check only the physical schema (column names, types) before execution, but do not sample data.

**Pros**: Fast. Catches obvious misconfigurations (missing columns, type mismatches).
**Cons**: Does not catch data-dependent structural errors: a refinement predicate that is always false due to a wrong assumption, a join key that never matches, a lookup version that does not exist. These are "structural" in nature but only detectable by processing data.

**Rejected because**: Insufficient. Schema validation (REQ-DATA-QUAL-003) is a necessary pre-check but does not replace data sampling for detecting data-dependent structural errors.

### Alternative 3: Progressive Processing with Adaptive Thresholds

Process data in waves (e.g., 1%, 10%, 100%) with escalating thresholds. If the first wave fails at > N%, halt. If it passes, expand to the next wave.

**Pros**: More representative than a fixed sample. Naturally scales with dataset size.
**Cons**: More complex to implement. Wave boundaries require checkpointing. The first 1% of an ordered dataset may not be representative of the whole. Checkpoint/rollback adds overhead and complexity.

**Rejected because**: Over-engineering for the use case. A fixed sample of 10K records is sufficient for detecting structural errors (which by definition affect most or all records). Progressive processing adds significant complexity.

## Consequences

**Positive**:
- Fast-fail for configuration errors (seconds instead of minutes/hours)
- Error Sink is not flooded with millions of identical structural errors
- Circuit breaker results are recorded in the accounting ledger for audit
- Configurable: sample size, threshold, enabled/disabled
- Clear diagnostic output (top 10 sample errors + failure rate)

**Negative**:
- Sampling overhead: 10K records must be processed before the full run
- False negatives: a structural error affecting < 5% of records may pass the circuit breaker (this is by design -- such errors are treated as data quality errors)
- False positives: an unrepresentative sample may incorrectly trigger the circuit breaker for a valid pipeline (rare for random sampling)

**Mitigations**:
- Circuit breaker is optional (can be disabled per-execution)
- Sample size and threshold are configurable for tuning
- Schema validation (REQ-DATA-QUAL-003) runs as a separate, independent pre-check before the circuit breaker

---

**Implements**: REQ-DATA-QUAL-001 (batch failure threshold), REQ-DATA-QUAL-002 (probabilistic circuit breaker)
