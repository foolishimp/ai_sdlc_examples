# ADR-011: Reverse-Join Table Strategy for Aggregation Adjoints

**Status**: Accepted
**Date**: 2026-02-20
**Feature**: REQ-F-CDME-001
**Drives**: REQ-F-ADJ-005, REQ-F-ADJ-008, REQ-F-ACC-004, REQ-NFR-PERF-003

---

## Context

For aggregation morphisms (groupBy/fold), the backward function must return all contributing source records via reverse-join table lookup (REQ-F-ADJ-005). Backward metadata must be captured during forward execution (REQ-F-ADJ-008). Any output record must be traceable back to source records without recomputation (REQ-F-ACC-004). Storage overhead must be bounded and configurable (REQ-NFR-PERF-003).

The core question: how to capture and store the mapping from each output group key to the set of contributing input record keys, efficiently, in a distributed execution context.

## Decision

During forward aggregation execution, capture a **reverse-join table** as a side-output dataset. The reverse-join table is a key-value mapping: `(output_group_key) -> Set[input_record_key]`. This dataset is persisted alongside the aggregation output within the same epoch context.

```scala
// During forward aggregation:
case class ReverseJoinEntry(
  outputGroupKey: GroupKey,
  inputRecordKey: RecordKey
)

// Storage: Parquet file co-located with output, partitioned by output_group_key
// Lookup: given output_group_key, read all input_record_keys

// Budget enforcement:
case class AdjointMetadataBudget(
  maxSizeBytes: Long,
  overflowStrategy: OverflowStrategy  // Reject | Compress | ExternalStorage
)
```

## Alternatives Considered

### Alternative 1: Inline Metadata (Embed in Output Records)

Embed contributing input keys directly in each output record as a nested array.

```scala
case class AggregatedRecord(
  groupKey: GroupKey,
  aggregatedValue: Value,
  contributingKeys: List[RecordKey]  // embedded
)
```

**Pros**: No separate storage. Lineage is immediately available with the output.
**Cons**: Dramatically increases output record size (an aggregate of 1M records would carry 1M keys per output row). Breaks Spark DataFrame schema conventions (nested arrays of unbounded size). Makes downstream consumption complex. Not compatible with standard analytics tools.

**Rejected because**: Output record bloat is unacceptable for large aggregations. Breaks DataFrame conventions.

### Alternative 2: Recomputation on Demand

Do not store reverse-join metadata. When backward traversal is requested, re-execute the forward pipeline and capture the mapping.

**Pros**: Zero storage overhead. No metadata management.
**Cons**: Backward traversal requires re-execution (expensive, potentially hours for large pipelines). Violates REQ-F-ACC-004 which requires backward traversal "without recomputation." Non-determinism risk if source data has changed since original execution.

**Rejected because**: Explicitly contradicts REQ-F-ACC-004 (no recomputation).

### Alternative 3: Bloom Filter Approximation

Store approximate reverse-join metadata using Bloom filters (probabilistic set membership).

**Pros**: Compact storage. Fast lookup. Configurable false positive rate.
**Cons**: Bloom filters have false positives (may report a key contributed when it did not). Cannot enumerate the full contributing key set. This violates the containment law: `backward(forward(records)) >= records` requires exact, not approximate, recall. Regulatory audit requires exact lineage.

**Rejected because**: Approximate lineage is insufficient for regulatory compliance (REQ-NFR-REG-001) and violates adjoint containment laws.

## Consequences

**Positive**:
- Exact reverse-join: every contributing input key is captured
- Backward traversal without recomputation
- Parquet storage is compact (columnar compression on key columns)
- Partitioned by output group key for efficient lookup
- Storage budget is configurable with overflow strategy

**Negative**:
- Storage overhead is O(n) where n is input record count (each input key appears once in the reverse-join table)
- Write amplification: aggregation produces both the output dataset and the reverse-join dataset
- Additional I/O during forward execution

**Mitigations**:
- Storage budget monitoring with configurable warnings and overflow strategies
- Parquet compression typically achieves 5-10x compression on key columns
- Reverse-join writes are parallel with output writes (minimal latency impact)
- For very large aggregations, the `ExternalStorage` overflow strategy offloads to cheaper storage

---

**Implements**: REQ-F-ADJ-005 (aggregation adjoint), REQ-F-ADJ-008 (backward metadata capture), REQ-F-ACC-004 (backward traversal proof), REQ-NFR-PERF-003 (storage bounds)
