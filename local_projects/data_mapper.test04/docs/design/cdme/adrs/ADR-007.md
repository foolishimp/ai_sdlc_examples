# ADR-007: Writer Monad for Telemetry

**Status**: Accepted
**Date**: 2026-02-20
**Feature**: REQ-F-CDME-001
**Drives**: REQ-F-TRV-005, REQ-NFR-SEC-001, REQ-NFR-MAINT-001

---

## Context

CDME must capture operational telemetry (row counts, quality metrics, latency) at every morphism application (REQ-F-TRV-005). Critically, telemetry must NOT affect transformation logic (REQ-NFR-SEC-001 requires pure business logic). Metrics must be exposed for operational monitoring (REQ-NFR-MAINT-001) with overhead under 5%.

The challenge is combining a pure computation (`A => B`) with a side-channel (`A => (B, TelemetryLog)`) without breaking purity guarantees.

## Decision

Use the **Writer monad pattern** to accumulate telemetry alongside transformation results. The telemetry log is a `Vector[TelemetryEntry]` monoid (concatenation with empty vector as identity).

```scala
case class TelemetryEntry(
  morphismId: MorphismId,
  rowCount: Long,
  nullRate: Double,
  typeViolationCount: Long,
  latencyMs: Long
)

case class Traced[A](value: A, telemetry: Vector[TelemetryEntry]):
  def map[B](f: A => B): Traced[B] = Traced(f(value), telemetry)
  def flatMap[B](f: A => Traced[B]): Traced[B] =
    val Traced(b, moreTelemetry) = f(value)
    Traced(b, telemetry ++ moreTelemetry)
```

Each morphism execution wraps its result in `Traced`, accumulating telemetry entries. At the end of a pipeline, the full telemetry log is extracted and exported.

## Alternatives Considered

### Alternative 1: Logging Framework (SLF4J / Logback)

```scala
val logger = LoggerFactory.getLogger(...)
def apply(record: Record): Record =
  val start = System.nanoTime()
  val result = transform(record)
  logger.info(s"morphism=$id rows=1 latency=${System.nanoTime() - start}")
  result
```

**Pros**: Familiar pattern. Rich ecosystem (appenders, formatters, log levels). No custom monad.
**Cons**: Logging is a side effect that violates purity (REQ-NFR-SEC-001). Log output is unstructured text requiring parsing. Logging performance in Spark executors can be unpredictable (I/O contention). Not composable -- cannot aggregate telemetry across morphisms without parsing logs.

**Rejected because**: Violates business logic purity requirement. Unstructured output is not suitable for accounting or monitoring integration.

### Alternative 2: Spark Accumulators

```scala
val rowCounter = spark.longAccumulator("rows")
def apply(record: Record): Record =
  rowCounter.add(1)
  transform(record)
```

**Pros**: Native Spark mechanism. Efficient for distributed counting.
**Cons**: Spark-specific (violates framework-agnostic model layer). Accumulators are mutable shared state. Limited to numeric aggregates (cannot capture structured telemetry entries). Not available at definition time for compiler telemetry.

**Rejected because**: Ties telemetry to Spark, violating the layered architecture (ADR-003). Mutable shared state contradicts purity requirements.

### Alternative 3: Aspect-Oriented Telemetry (Interceptors)

```scala
class TelemetryInterceptor extends MorphismInterceptor:
  def around[A, B](morphism: Morphism[A, B], input: A): B =
    val start = System.nanoTime()
    val result = morphism.apply(input)
    record(morphism.id, System.nanoTime() - start)
    result
```

**Pros**: Non-invasive; morphisms do not need to know about telemetry. Clean separation.
**Cons**: Interceptors are an imperative pattern with hidden side effects. Composition of interceptors is ordering-dependent. Hard to test (requires mocking the interceptor). The "hidden" nature contradicts the principle that all effects should be visible in the type signature.

**Rejected because**: Hidden side effects contradict the design principle of effects being visible in types. Interceptors are imperative and ordering-dependent.

## Consequences

**Positive**:
- Telemetry is visible in the type signature (`Traced[A]` instead of `A`)
- Pure: the `Traced` wrapper is a value, not a side effect
- Composable: `flatMap` naturally accumulates telemetry across morphism chains
- Framework-agnostic: works identically in Spark, Flink, or single-threaded tests
- Testable: telemetry output can be asserted in unit tests

**Negative**:
- Additional wrapping overhead (allocating `Traced` and `Vector` per step)
- Developers must use `map`/`flatMap` instead of direct function calls
- `Vector` concatenation is O(log n); for very long chains, consider periodic flush

**Mitigations**:
- The overhead target is < 5% of pipeline latency; `Vector` append is efficient for typical chain lengths
- At Spark integration boundary, telemetry is extracted and flushed to accumulators for efficiency

---

**Implements**: REQ-F-TRV-005 (operational telemetry), REQ-NFR-SEC-001 (pure business logic), REQ-NFR-MAINT-001 (monitoring)
