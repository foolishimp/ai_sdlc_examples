# ADR-001: Scala 3 Opaque Types for Domain Modelling

**Status**: Accepted
**Date**: 2026-02-20
**Feature**: REQ-F-CDME-001
**Drives**: REQ-F-LDM-004, REQ-F-TYP-005, REQ-F-PDM-002

---

## Context

CDME has many domain-specific value types that wrap primitives: `Grain` (wraps a string level like "Atomic"), `Epoch` (wraps an identifier), `SemanticType` tags (like "Money", "ISIN"), `EntityId`, `MorphismId`, etc. These types need to be:

1. Distinguishable at compile time (prevent mixing `EntityId` with `MorphismId`)
2. Zero overhead at runtime (no boxing on the JVM)
3. Compatible with Spark serialisation (must resolve to primitive JVM types in DataFrames)
4. Ergonomic for API users

The requirements specify grain metadata on every entity (REQ-F-LDM-004) and semantic type distinctions (REQ-F-TYP-005). Both need compile-time safety without runtime cost.

## Decision

Use **Scala 3 opaque types** for domain value types that wrap primitives.

```scala
object GrainTypes:
  opaque type Grain = String
  object Grain:
    def apply(level: String): Grain = level
  extension (g: Grain)
    def level: String = g

object EpochTypes:
  opaque type EpochId = String
  object EpochId:
    def apply(id: String): EpochId = id

object SemanticTags:
  opaque type SemanticTag = String
  object SemanticTag:
    def apply(name: String): SemanticTag = name
```

## Alternatives Considered

### Alternative 1: Value Classes (`extends AnyVal`)

```scala
case class Grain(level: String) extends AnyVal
```

**Pros**: Familiar from Scala 2; some boxing avoidance.
**Cons**: Boxing still occurs in generic contexts, collections, and pattern matching. Spark serialisation requires unwrapping. Only works for single-field wrappers. Scala 3 deprecates `AnyVal` in favour of opaque types.

**Rejected because**: Inconsistent boxing behaviour; deprecated pattern in Scala 3.

### Alternative 2: Tagged Types (Shapeless / Iron)

```scala
type Grain = String @@ GrainTag
```

**Pros**: Library-proven pattern; works with Scala 2 and 3.
**Cons**: Adds external library dependency to the model layer (violating the zero-dependency constraint). Type inference can be surprising. Spark integration requires custom encoders.

**Rejected because**: External dependency in model layer violates project constraint "Model layer has zero external dependencies."

### Alternative 3: Plain Case Classes

```scala
case class Grain(level: String)
```

**Pros**: Simple, familiar, pattern-matchable.
**Cons**: Runtime boxing for every instance. Memory overhead in large datasets. Spark requires custom encoders or UDTs. Not zero-cost.

**Rejected because**: Runtime overhead unacceptable for types used per-record in distributed datasets.

## Consequences

**Positive**:
- Zero runtime overhead (opaque types erase to their underlying type at JVM level)
- Compile-time type safety (cannot mix `Grain` with `EpochId`)
- Spark-friendly (underlying JVM type is directly serialisable)
- No external dependencies

**Negative**:
- Opaque types cannot be pattern-matched (must use explicit accessor methods)
- Extension methods required for all operations (slightly more boilerplate)
- Developers less familiar with opaque types may find the pattern unfamiliar

**Risks**:
- If Spark custom encoders are needed for complex opaque types, the zero-cost benefit may be partially negated at the serialisation boundary

---

**Implements**: REQ-F-LDM-004 (grain metadata), REQ-F-TYP-005 (semantic types), REQ-F-PDM-002 (generation grain)
