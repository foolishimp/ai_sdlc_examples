# ADR-002: Sealed ADT for Type System Encoding

**Status**: Accepted
**Date**: 2026-02-20
**Feature**: REQ-F-CDME-001
**Drives**: REQ-F-TYP-001, REQ-F-TYP-002, REQ-F-TYP-003, REQ-F-TYP-004, REQ-F-TYP-005, REQ-BR-TYP-001

---

## Context

CDME requires an extended type system supporting primitives, sum types, product types, refinement types, and semantic types (REQ-F-TYP-001). Types must unify during morphism composition (REQ-F-TYP-004), support subtype relationships (declared, not inferred), and support nested composition (e.g., `Option[List[(A, B)]]`).

The type system representation must:
1. Be exhaustively matchable (compiler checks all cases)
2. Support recursive/nested composition
3. Be serialisable (for execution artifacts and lineage)
4. Support runtime type checking for refinement predicates
5. Work with Spark's type system at the integration boundary

## Decision

Use a **sealed trait hierarchy (ADT)** for the `CdmeType` type system. Subtype relationships are **nominal** (declared in a `SubtypeRegistry`) rather than structural.

```scala
sealed trait CdmeType

enum PrimitiveKind:
  case StringType, IntType, LongType, DecimalType, DateType, TimestampType, BooleanType

case class PrimitiveType(kind: PrimitiveKind) extends CdmeType
case class SumType(variants: List[CdmeType], discriminant: String) extends CdmeType
case class ProductType(fields: Map[String, CdmeType]) extends CdmeType
case class RefinementType(base: CdmeType, predicate: String, check: Any => Boolean) extends CdmeType
case class SemanticType(base: CdmeType, tag: SemanticTag) extends CdmeType
case class OptionType(inner: CdmeType) extends CdmeType
case class ListType(inner: CdmeType) extends CdmeType

class SubtypeRegistry:
  def declare(sub: CdmeType, sup: CdmeType): Unit
  def isSubtypeOf(sub: CdmeType, sup: CdmeType): Boolean
```

## Alternatives Considered

### Alternative 1: Scala 3 Union Types

```scala
type CdmeType = PrimitiveType | SumType | ProductType | RefinementType | SemanticType
```

**Pros**: Lighter syntax; native Scala 3 feature; no sealed trait boilerplate.
**Cons**: Union types are not exhaustively checked in pattern matching (compiler warns but does not error). Cannot add methods to the union. Cannot be serialised without a custom codec that re-discriminates. Spark has no concept of Scala union types.

**Rejected because**: Non-exhaustive matching is a safety concern for a type system that must reject all ambiguous compositions. Serialisation complexity is unacceptable for a core domain type.

### Alternative 2: Typeclass-Based (Cats/Shapeless)

```scala
trait TypeRepr[T]:
  def unify(other: TypeRepr[_]): Option[TypeRepr[_]]
```

**Pros**: Maximally extensible; each type implements its own logic; composable.
**Cons**: Type erasure means runtime type checking is complex. Hard to serialise. Requires external library (Cats or Shapeless) in the model layer. Overly abstract for a bounded set of type constructors.

**Rejected because**: External dependencies in model layer; over-engineering for a known, fixed set of type constructors.

### Alternative 3: Structural Subtyping (Row Polymorphism)

Instead of nominal subtype declarations, use structural matching (a type is a subtype if it has all the same fields).

**Pros**: More flexible; no need for explicit subtype declarations.
**Cons**: Harder to reason about; surprising subtype relationships. A `ProductType{name: String, age: Int}` would be a subtype of `ProductType{name: String}`, which may not be intended. Structural subtyping is harder to implement correctly and harder to debug.

**Rejected because**: Requirements state "subtype relationships are declared, not inferred" (REQ-F-TYP-004). Structural subtyping contradicts this.

## Consequences

**Positive**:
- Exhaustive pattern matching ensures all type cases are handled
- Straightforward serialisation (JSON ADT with type discriminant)
- Nominal subtyping is explicit and auditable
- Nested composition is natural (types reference other CdmeType instances)
- Refinement predicates stored as both executable function and string description

**Negative**:
- Adding a new type constructor requires modifying the sealed hierarchy and all match expressions
- Refinement predicate functions are Scala lambdas and cannot be serialised; only the string description is persisted (the executable function must be reconstructed from source)

**Risks**:
- If the set of type constructors grows significantly, the sealed hierarchy becomes unwieldy. Mitigated by the fact that type constructors in type theory are a well-understood, bounded set.

---

**Implements**: REQ-F-TYP-001 (extended types), REQ-F-TYP-002 (refinement), REQ-F-TYP-003 (no implicit casting), REQ-F-TYP-004 (unification), REQ-F-TYP-005 (semantic), REQ-BR-TYP-001 (explicit conversion)
