# ADR-013: Scala 2.13 + Spark 3.5 (Supersedes Scala 3 Choice)

**Status**: Accepted
**Date**: 2026-02-20
**Feature**: REQ-F-CDME-001
**Supersedes**: ADR-001 (Scala 3 Opaque Types)
**Drives**: REQ-NFR-PERF-001, REQ-NFR-PERF-002, REQ-F-INT-001

---

## Context

CDME was designed with Scala 3.3.3, leveraging opaque types, givens, extensions, and enums (ADR-001). However, Apache Spark 3.5.x is built against Scala 2.12/2.13 and does **not** support Scala 3:

- Spark's macro-heavy Dataset API (encoders, implicits) fails across the Scala 2/3 boundary
- `CrossVersion.for3Use2_13` allows pulling Spark jars into a Scala 3 project but is fragile — encoder derivation, implicit resolution, and catalyst expressions break at runtime
- Spark 4.0 is expected to add Scala 3 support but is not yet released

REQ-NFR-PERF-001 (distributed execution via Spark) is a Critical requirement. The Spark integration module (`cdme-spark`) is not optional — it is the production execution engine.

## Decision

Switch the entire CDME project from **Scala 3.3.3 to Scala 2.13.12** with native Spark 3.5.0 support.

### Migration Impact

| Scala 3 Feature | Scala 2.13 Equivalent |
|-----------------|----------------------|
| `opaque type Grain = String` | `case class Grain(value: String) extends AnyVal` or tagged type |
| `given`/`using` | `implicit val`/`implicit` parameter |
| `extension (x: T)` | `implicit class TExtensions(x: T)` |
| `enum Color { case Red, Blue }` | `sealed trait Color; object Color { case object Red extends Color; ... }` |
| Top-level definitions | Wrap in `object` |
| `export` clauses | Manual re-exports |
| Union types `A | B` | `Either[A, B]` or sealed trait |

## Alternatives Considered

### Alternative 1: Stay on Scala 3, Stub Spark

Keep Scala 3.3.3 for model/compiler/runtime/api. Leave `cdme-spark` as a stub until Spark 4.0.

**Pros**: Preserves opaque types, givens, modern syntax.
**Cons**: Cannot run any distributed execution. Cannot validate NFR-PERF requirements. The core value proposition (category-theoretic data mapping *at scale*) is untestable.

**Rejected because**: A data mapping engine that can't map data is a spec, not a system.

### Alternative 2: Wait for Spark 4.0

Pause until Spark 4.0 RC with Scala 3 support is available.

**Pros**: Best of both worlds eventually.
**Cons**: Unknown timeline. May introduce new compatibility issues. Blocks all progress.

**Rejected because**: Indefinite timeline, blocks validation.

### Alternative 3: Dual-Build (Scala 2.13 for Spark modules, Scala 3 for core)

Use sbt cross-compilation: core modules in Scala 3, Spark integration in Scala 2.13.

**Pros**: Core code stays modern; Spark code works natively.
**Cons**: Cross-version publishing complexity. All inter-module APIs must avoid Scala 3-only features. Essentially requires writing the API surface in Scala 2.13 anyway. Increases build complexity significantly.

**Rejected because**: The inter-module API surface IS most of the code. Net benefit is marginal for significant build complexity.

## Consequences

**Positive**:
- Native Spark 3.5 integration — Dataset API, encoders, catalyst all work
- Can run distributed tests immediately
- Broader talent pool familiar with Scala 2.13
- Simpler build tooling (no cross-version workarounds)

**Negative**:
- Loss of opaque types — value classes (`extends AnyVal`) have inconsistent boxing in generics/collections (ADR-001's original concern)
- Loss of `given`/`using` — implicit syntax is more verbose
- Loss of enums — sealed trait hierarchies are more boilerplate
- Loss of extension methods — implicit classes are slightly less ergonomic
- ADR-001 is now incorrect — domain types will use value classes or tagged types

**Mitigations**:
- For domain types: use `extends AnyVal` where possible; accept boxing in generic contexts (the performance impact is in the Spark shuffle, not in wrapper boxing)
- For implicits: follow established Scala 2.13 implicit patterns; the team will be familiar with these
- Consider re-migrating to Scala 3 when Spark 4.0 is stable

**Risks**:
- Value class boxing in generic contexts (e.g., `List[Grain]`) adds minor GC pressure. Mitigation: benchmark and optimize hot paths if needed.

---

## Methodology Observation

This ADR was written during the **build/environment setup** phase — after design→code had already generated 71 Scala 3 files. The Scala 3 / Spark incompatibility should have been caught during the **requirements→design** edge, where the ecosystem binding (Scala version + Spark version) is decided.

**Root cause**: The design edge's evaluator checklist did not include an **ecosystem compatibility check** — a verification that the chosen language version, framework versions, and runtime dependencies form a coherent, buildable stack. The `design_is_implementable` evaluator check is too coarse; it validates architectural feasibility but not dependency-graph compatibility.

**Recommendation for methodology**: Add an `ecosystem_compatibility` evaluator to the requirements→design edge config:
- Verify language version supports all declared framework dependencies
- Verify framework versions are mutually compatible (e.g., Spark + Scala, Spring Boot + Java)
- Verify build tool supports the chosen language version
- Flag when a design decision creates a version lock-in that constrains other choices

This gap was identified as TELEM-004 (multi-module build vs flat source layout) but the deeper issue — **language/framework version incompatibility** — was not caught by any evaluator or source finding. The three-direction gap detection found the *symptom* (layout mismatch) but not the *cause* (Scala 3 doesn't work with Spark 3.5).

---

**Implements**: REQ-NFR-PERF-001 (distributed execution), REQ-NFR-PERF-002 (skew mitigation), REQ-F-INT-001 (synthesis engine on Spark)
